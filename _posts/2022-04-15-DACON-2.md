---
title: "뉴스 그룹 분류 경진대회"
categories:
  - DACON
feature_text: |
  The History of the DACON
---

## 뉴스 그룹 분류 경진대회 (2022.04.04 ~ 2022.04.15)

![image](https://user-images.githubusercontent.com/26592315/164354819-9ca46cec-b537-4de8-846e-670d9b990b80.png)


- 주최 : DACON

- 목적 : 20개의 카테고리로 구분되는 영어 뉴스 데이터셋을 이용해 뉴스 그룹을 분류

- 최종 순위 : 10위  ( 403명 참가)

- Private 10 위 ( 0.77517 )  

![image](https://user-images.githubusercontent.com/26592315/164357121-6be7c5f9-3969-4630-a5c7-10938b01d474.png)


---
 
- [나의 코드 리뷰](#나의-코드-리뷰)

- [남의 코드 리뷰](#남의-코드-리뷰)

---

## 나의 코드 리뷰


- Data

![image](https://user-images.githubusercontent.com/26592315/164359514-3ab7ee01-2c27-49da-bdb3-82d233b9d1a5.png)

- Notepad++ 으로 데이터 확인 

- columns 설명
  - id : index
  - text : 뉴스 텍스트
  - target : 뉴스 그룹 종류 번호

- target
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

- EDA
  ![image](https://user-images.githubusercontent.com/26592315/164575867-bcaa5614-46bc-450a-9a5c-b79fecf9b1d4.png)

  - 다소 골고루 뉴스 그룹이 분포되어 있다.

  - 뉴스 텍스트의 평균 길이 : 1163.0

  - 결측치 없음.

  - train 데이터 행 개수 : 9233

  - label 9 인 데이터 시각화

  ![image](https://user-images.githubusercontent.com/26592315/164578150-fbc9a1ea-573d-4db2-a64a-2dd30e72bf4e.png)

  ![image](https://user-images.githubusercontent.com/26592315/164578172-786e6c90-c773-48dc-9891-5af010c649b8.png)

  - label 13 인 데이터 시각화

  ![image](https://user-images.githubusercontent.com/26592315/164578379-ac64a0a2-6636-48c5-a238-96155c2b9123.png)

  ![image](https://user-images.githubusercontent.com/26592315/164578392-b9ac6848-3333-43e4-9d97-651716cbaf31.png)


  - label 14 인 데이터 시각화

  ![image](https://user-images.githubusercontent.com/26592315/164578637-d4ae9249-fea1-40d3-a0f7-cadd54ac0f46.png)

  ![image](https://user-images.githubusercontent.com/26592315/164578651-16047e92-46ab-4933-be19-e4e299a3bb55.png)



- 뉴스 텍스트 데이터를 분류할 때 가장 중요한 점은 텍스트 전처리, 불용어, 학습 모델 선정입니다.

---

- 텍스트 전처리
  - 데이터에서 특수문자도 분류를 하기에 특수문자를 제거하지는 않고 space 제거에 초점을 맞추었음.

---

- 불용어

  ```python
  stopwords = [
      "a", "about", "above", "across", "after", "afterwards", "again", "against",
      "all", "almost", "alone", "along", "already", "also", "although", "always",
      "am", "among", "amongst", "amoungst", "amount", "an", "and", "another",
      "any", "anyhow", "anyone", "anything", "anyway", "anywhere", "are",
      "around", "as", "at", "back", "be", "became", "because", "become",
      "becomes", "becoming", "been", "before", "beforehand", "behind", "being",
      "below", "beside", "besides", "between", "beyond", "bill", "both",
      "bottom", "but", "by", "call", "can", "cannot", "cant", "co", "con",
      "could", "couldnt", "cry", "de", "describe", "detail", "do", "done",
      "down", "due", "during", "each", "eg", "eight", "either", "eleven", "else",
      "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone",
      "everything", "everywhere", "except", "few", "fifteen", "fifty", "fill",
      "find", "fire", "first", "five", "for", "former", "formerly", "forty",
      "found", "four", "from", "front", "full", "further", "get", "give", "go",
      "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter",
      "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his",
      "how", "however", "hundred", "i", "ie", "if", "in", "inc", "indeed",
      "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter",
      "latterly", "least", "less", "ltd", "made", "many", "may", "me",
      "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly",
      "move", "much", "must", "my", "myself", "name", "namely", "neither",
      "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone",
      "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on",
      "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our",
      "ours", "ourselves", "out", "over", "own", "part", "per", "perhaps",
      "please", "put", "rather", "re", "same", "see", "seem", "seemed",
      "seeming", "seems", "serious", "several", "she", "should", "show", "side",
      "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone",
      "something", "sometime", "sometimes", "somewhere", "still", "such",
      "system", "take", "ten", "than", "that", "the", "their", "them",
      "themselves", "then", "thence", "there", "thereafter", "thereby",
      "therefore", "therein", "thereupon", "these", "they", "thick", "thin",
      "third", "this", "those", "though", "three", "through", "throughout",
      "thru", "thus", "to", "together", "too", "top", "toward", "towards",
      "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us",
      "very", "via", "was", "we", "well", "were", "what", "whatever", "when",
      "whence", "whenever", "where", "whereafter", "whereas", "whereby",
      "wherein", "whereupon", "wherever", "whether", "which", "while", "whither",
      "who", "whoever", "whole", "whom", "whose", "why", "will", "with",
      "within", "without", "would", "yet", "you", "your", "yours", "yourself",
      "yourselves"]

  ```

  - 불용어 처리가 텍스트 데이터를 분류하기에 중요하다. 
  - stopwords.words('english')으로도 불용어 처리가 가능하지만 성능은 위의 stopwords가 더 좋다.

---

- 학습 
  - [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)

  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2))
  #  min_df=2, preprocessor=preprocessor, 

  vectorizer.fit(X) 
  X = vectorizer.transform(X) 
  ```
  - 모델을 비교할 때에는 train_test_split을 했지만 실제로 제출할 때에는 train 데이터 전체 이용
  - X_train을 학습한 모델이 아닌 train(X) 데이터 전체를 학습시킴. ( 과적합의 우려보다 데이터의 양이 많지가 않아서 오히려 점수가 올라감 )

---

- 모델

  ```python
  from sklearn.neural_network import MLPClassifier
  model = MLPClassifier(max_iter=500) 
  ```
  - 다중 분류, 다중 label 분류를 지원하는 MLPClassifier를 사용했습니다.

---

## 대회 회고

컴퓨터는 일반적으로 문장(텍스트)을 계산할 수 없고 숫자형 피처를 데이터로 입력받아 동작하기 때문에

텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수가 없습니다.

따라서 컴퓨터가 계산을 하기 위해서는 먼저 문장을 숫자형 값으로 바꾼 다음 계산하도록 해야 합니다.

이것을 워드 임베딩(word embedding)이라고 합니다.

TfidfVectorizer 기법을 사용해 문장을 숫자로 표현해보겠습니다.

TfidfVectorizer 는 입력된 문장을 토큰화(Tokenize)하여 토큰의 등장 빈도 벡터로 바꿔주는 기법인 CountVectorizer에다가 단어 빈도 가중치를 부여한 것입니다.

- TfidfVectorizer

  Tf, idf 두 가지를 먼저 이해해야 합니다.

  1) Tf(Term Frequency)

  하나의 문서(문장)에서 특정 단어가 등장하는 횟수

  2) Idf(Inverse Document Frequency)

  Df(Document Frequency)는 문서 빈도. 특정 단어가 몇 개의 문서(문장)에서 등장하는지를 수치화 한 것. 그것의 역수가 idf다. 보통 그냥 역수를 취하기 보다는 아래처럼 수식화한다. 역수 개념을 사용하는 이유는, 적은 문서(문장)에 등장할수록 큰 숫자가 되게하고 반대로 많은 문서(문장)에 등장할수록 숫자를 작아지게 함으로써 여러 문서(문장)에 의미 없이 사용되는 단어의 가중치를 줄이기 위함.

  Tf-idf 수치는 Tf 값과 Idf 값을 곱하여 구한다. 해당 연산을 거친 최종 Tf-idf 값은 0과 1사이로 만들어진다.
  >- TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법

단어들의 출현 빈도로 이루어진 벡터를 BoW 라고 합니다.

TfidfVectorizer.fit(text)를 통해 text가 가지고 있는 모든 단어를 BoW로 구성하고, 이 단어들에 대해 Tf-idf 값을 계산한 뒤 각 단어의 인덱스 위치에 Tf-idf 값이 들어간 벡터가 만들어집니다. 이 과정에서 CountVectorizer와 마찬가지로 I, a 등 한 글자 단어는 사라지고 특정 단어를 가지고 있지 않다면 Tf = 0 이므로 Tf-idf도 0으로 표현됨을 알 수 있습니다.

이렇게 임베딩을 마치고 나면, 아래와 같이 문서(문장) 간 유사도도 수치화할 수 있습니다.

모든 문서에 포함된 모든 단어를 이용해 BoW를 구성할 경우, 각 벡터들이 sparse해질 가능성이 존재하고 max_features = n 으로 설정할 경우 모든 문서를 기준으로 가장 많이 나온 상위 n개의 단어만을 사용해 임베딩을 진행합니다. ( vocabulary = None 일 경우만 적용 )

- TfidfVectorizer의 Pros

  1. CountVectorizer가 갖는 단점 보완(자주 등장하는 단어의 가중치 축소)

  2. 0과 1사이의 수치로 크면 자주, 작으면 덜 자주 등장한다는 것을 직관적으로 알 수 있음

 

- TfidfVectorizer의 Cons

  1. 데이터 양이 크고 많은 경우, BoW의 length가 커 sparse한 벡터 표현이 만들어질 가능성이 큼

  -> 전처리, stop_words, max_features 등을 이용해 잘 처리해야함

  2. 차원이 너무 큰 벡터가 만들어질 가능성 존재 


## 남의 코드 리뷰


---
0.77517
- Private 2위 ( 0.78774 )  

  ```python
  # 학습
  TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)

  #분류 모델
  SGDClassifier(loss='squared_hinge', penalty='l2',alpha=1e-4, max_iter=5)
  ```
  - 차이점:
    - 분류 모델을 SGDClassifier을 사용했어야했다... 더 성능이 좋은 모델을 찾기 위해서는 여러가지 시도를 해봤어야 했다. 모델의 중요성 인지!

---

- Private 3위 ( 0.78145 )
  ```python
  # 학습 
  TfidfVectorizer(ngram_range=(1, 2), stop_words='english')

  k = 15
  skfold = StratifiedKFold(k, shuffle=True, random_state=SEED)
  CNB = ComplementNB(alpha=0.2) ## 모델 생성
  
  # 분류 모델
  def OOF_predict(model, X, y, test, SKfold):
    """
    Out-Of-Fold 방식을 통해 test를 예측
    """ 
    model_valid_score = []
    model_predict = np.zeros(shape=(X.shape[0], len(y.unique())))

    for train_idx, valid_idx in SKfold.split(X, y):
        X_train, y_train = X[train_idx], y[train_idx]
        X_valid, y_valid = X[valid_idx], y[valid_idx]

        model.fit(X_train, y_train)

        y_predict = model.predict_proba(X_valid).argmax(-1) ## 각 클래스별 확률(valid)을 구하고 -1차원을 기준으로 argmax를 구한다(확률이 가장 높은 인덱스 번호를 반환)
        test_predict = model.predict_proba(test)  ## 각 클래스별 확률(test)을 구함

        model_valid_score.append(accuracy(y_predict, y_valid)) ## valid 정확도
        model_predict += test_predict / SKfold.n_splits ## test 예측값(k Fold)
        
    model_predict = model_predict.argmax(-1)              
    return model_valid_score, model_predict

    CNB_valid_acc, CNB_test_pred = OOF_predict(CNB, train_tfidf, target, test_tfidf, skfold)

    np.mean(CNB_valid_acc)
  ```
  - ComplementNB + OOF 앙상블


