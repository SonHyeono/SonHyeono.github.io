---
title: "Crawling 시작"
categories:
  - Crawling( 크롤링 )
feature_text: |
  The History of the Crawling
---

## Crawling과 Scraping

Crawling과 Scraping의 차이

| Data Scraping | Data Crawling|
|----|----|
|Invloves extracting data from various sources including web|Refers to downloading pages from the web|
|Can be done at any scale|Mostly done at a large scale|
|Deduplication is not necessarily a part|Deduplication is an essential part|
|Needs crawl agent and parser | Needs only crawl agent|

- Scraping : 웹의 다양한 소스에서 특정 데이터를 추출
- Crawling : 웹페이지에 있는 데이터 모두 긁어오기


python을 이용한 Crawling(크롤링) 도구들:
1. Scrapy (프레임워크)
2. Beautiful Soup (라이브러리)
3. Selenium (라이브러리)

- Scrapy 

    크롤링 작업을 비동기 방식으로 병렬처리 해주기에 크롤링을 해야하는 사이트가 많거나, 여러 페이지를 신속하게 파싱해야 하는경우, 처리속도상의 큰 이점이 있다. 
    
    ( 병렬로 크롤링이 진행되기 때문에, 크롤링 된 데이터의 순서가 뒤바뀌거나, 예상한 순서대로 코드가 작동하지 않을 수 있기 때문에 난이도가 있다.) 
    
    Scrapy에서 제안하는 가이드라인을 따르는 것이 중요하고 자유도가 떨어지는 것이 단점.
    
    `비동기 네트워킹 라이브러리인 Twisted에 built on 되어, 퍼포먼스가 매우 뛰어나고 확장성이 뛰어남`


- Beautiful Soup
    
    간편한 만큼, 기능이 부족하여 복잡한 크롤러를 만들기 위해서는 일일이 모든 기능을 손으로 구현해야 하고 전체적인 데이터 파이프라인을 직접 설계.

    BeautifulSoup은 라이브러리에 불과해서 기본적인 Python의 구조대로 싱글프로세스로 작동하기에 Scrapy에 비하여 성능이 많이 뒤쳐지는 편이며, 병렬처리를 위해서는 별도로 multiprocessing과 같은 병렬처리 라이브러리를 사용하여 직접 설계를 해주어야 한다는 치명적인 단점.
    
    `속도를 향상시키려면 multiprocessing import 필요, 생태계 연약`

- Selenium

    원래 Selenium 웹개발자들이 웹을 개발하는 과정에서, 웹사이트를 테스트 하는 용도로 사용되는 라이브러리이다. 사이트 규모가 커지면, 사람이 일일이 모든 url을 들어가서 테스트 해볼 수 없기 때문에, 테스트 자동화를 위하여 사용하는 도구인데, 이를 활용하여 크롤링에도 사용할 수 있다. 

    특히, 웹사이트가 동적 웹페이지(dynamic webpage)로 구성된 경우에는, Selenium이 유용하게 쓰일 수 있다. 예를 들어, 웹페이지가 자바스크립트로 설계되어, 사용자가 웹 브라우저 상에서 보고 있는 정보와, 페이지의 원본소스가 상이한 경우에는 원본페이지의 html 추출로는 원하는 정보의 크롤링이 불가능한 경우가 있다. 이런 경우, Selenium을 이용하면 페이지에서 동적처리가 끝난 페이지소스를 다운받아 크롤링 할 수 있다. 다만 Selenium은 실제 구글 크롬과 같은 웹브라우저를 실행하여 작동되는 구조이기 때문에 속도가 느린 편이고 메모리와 CPU점유율도 많이 차지하여 효율성이 낮다는 단점또한 존재한다. 따라서, 동적 웹페이지의 크롤링이 필요하지 않은 이상, Sellenium의 사용은 최대한 자제하는 것이 좋다.
 
    `CPU와 메모리 사용량이 상대적으로 많고, 긁어오려는 컨텐츠가 HTML보다는 JavaScript로 문서에 더해진 경우에 적합( 긁어올 데이터가 많은 ajax/pjax 리퀘스트 후에 나타나는 경우라면 selenium을 쓰는 걸 권장 )`

---

Scraping이 가능하도록 지원해주는 도구들:
1. Selenium + 브라우저 (Java, Python, C#)
2. PhantomJS (JavaScript, Selenium)
3. Splash + Scrapy (Python)

- Splash + Scrapy
    
    ( Scrapy는 lxml을 기반으로 만들어져 있어서 자바스크립트를 읽고 실행하지 못함. )

    따라서 자바스크립트를 처리하기 위해 Selenium과 PhantomJS를 주로 사용하는데, Scrapy에서도 Splash를 사용하면 자바스크립트를 처리할 수 있다.
    
    ( Splash는 웹킷을 기반으로 만들들어진 헤드리스 브라우저를 사용한 서버 )

    Javascript 렌더링을 위해 Splash를 사용하고, Splash가 가져온 Javascript rendered DOM 객체를 scrapy-splash가 받아서 Scrapy에 제공한다. 
    
    Scrapy에서는 크롤링 요청에 따른 응답을 가져올 middleware로 기존의 것 대신에 scrapy-splash를 선택해 사용하는 구조이다. ( Splash는 클라이언트와 서버의 중간에서 양쪽을 중계하며, 페이지 내부의 자바스크립트를 실행해 자바스크립트의 onload 이벤트가 발생한 시점의 DOM 트리를 HTML로 응답한다. )

    [사용법 URL](https://github.com/scrapy-plugins/scrapy-splash)

    동작 과정
    1. 클라이언트 -> Splash : 요청 전송

    2. Splash -> 서버: 요청 전송

    3. 서버 -> Splash: HTML 응답

    4. Splash: 페이지 내부의 자바스크립트 실행

    5. Splash -> 클라이언트: 자바스크립트 실행 후의 HTML 응답

    

    

    

    







   







